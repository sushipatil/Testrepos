{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95c545d",
   "metadata": {},
   "source": [
    "You are part of a team developing a text classification system for a news aggregator \n",
    "platform. The platform aims to categorize news articles into different topics automatically. \n",
    "The dataset contains news articles along with their corresponding topics. Perform only the \n",
    "Feature extraction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917eb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203036e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20030219  aba decides against community broadcasting lic...\n",
      "1      20030219     act fire witnesses must be aware of defamation\n",
      "2      20030219     a g calls for infrastructure protection summit\n",
      "3      20030219           air nz staff in aust strike for pay rise\n",
      "4      20030219      air nz strike to affect australian travellers\n",
      "Unique Topics:  ['aba decides against community broadcasting licence'\n",
      " 'act fire witnesses must be aware of defamation'\n",
      " 'a g calls for infrastructure protection summit' ...\n",
      " 'carnaby cockatoo revival program'\n",
      " 'collier convinced colin barnett has overwhelming support'\n",
      " 'cowboys offer indigenous kids north queensland hope for future']\n",
      "Topic Distribution: \n",
      " national rural news                                               808\n",
      "abc sport                                                         718\n",
      "abc weather                                                       714\n",
      "abc business news and market analysis                             585\n",
      "abc entertainment                                                 551\n",
      "                                                                 ... \n",
      "palestinian militants killed in missile attack                      1\n",
      "parties debate election spending levels                             1\n",
      "pasta plight drought hits hard                                      1\n",
      "pedestrian injured in hit and run                                   1\n",
      "cowboys offer indigenous kids north queensland hope for future      1\n",
      "Name: headline_text, Length: 1021130, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('news.csv')\n",
    "\n",
    "# Explore the first few rows to understand the data\n",
    "print(df.head())\n",
    "\n",
    "# Check the unique topics/categories\n",
    "unique_topics = df['headline_text'].unique()\n",
    "print(\"Unique Topics: \", unique_topics)\n",
    "\n",
    "# Check the distribution of articles across topics\n",
    "topic_distribution = df['headline_text'].value_counts()\n",
    "print(\"Topic Distribution: \\n\", topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag-of-Words(BOW)Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b660c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      " ['aba' 'act' 'against' 'ambitious' 'aware' 'be' 'broadcasting' 'community'\n",
      " 'decides' 'defamation' 'fire' 'jump' 'licence' 'must' 'of' 'olsson'\n",
      " 'triple' 'wins' 'witnesses']\n",
      "\n",
      "BOW Matrix:\n",
      "\n",
      "[[1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=['aba decides against community broadcasting licence','act fire witnesses must be aware of defamation',\n",
    "      'ambitious olsson wins triple jump']\n",
    "#Initialize the vector\n",
    "vectorizer=CountVectorizer()\n",
    "#transform\n",
    "x=vectorizer.fit_transform(text)\n",
    "#get feature names(words)\n",
    "feature_names=vectorizer.get_feature_names_out()\n",
    "#Display BOW-Matrix\n",
    "print('Feature names:\\n',feature_names)\n",
    "print()\n",
    "print('BOW Matrix:\\n')\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ae100",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62925db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names(TF-IDF):\n",
      " ['aba' 'act' 'against' 'ambitious' 'aware' 'be' 'broadcasting' 'community'\n",
      " 'decides' 'defamation' 'fire' 'jump' 'licence' 'must' 'of' 'olsson'\n",
      " 'triple' 'wins' 'witnesses']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.40824829 0.         0.40824829 0.         0.         0.\n",
      "  0.40824829 0.40824829 0.40824829 0.         0.         0.\n",
      "  0.40824829 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.35355339 0.         0.         0.35355339 0.35355339\n",
      "  0.         0.         0.         0.35355339 0.35355339 0.\n",
      "  0.         0.35355339 0.35355339 0.         0.         0.\n",
      "  0.35355339]\n",
      " [0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.4472136  0.4472136  0.4472136\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text=['aba decides against community broadcasting licence','act fire witnesses must be aware of defamation',\n",
    "      'ambitious olsson wins triple jump']\n",
    " \n",
    "tfidf=TfidfVectorizer()\n",
    "x_tfidf=tfidf.fit_transform(text)\n",
    "features_names_tfidf=tfidf.get_feature_names_out()\n",
    "print('Feature Names(TF-IDF):\\n',features_names_tfidf)\n",
    "print()\n",
    "print('TF-IDF Matrix:\\n',x_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c51d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\sushmita\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in e:\\sushmita\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in e:\\sushmita\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\sushmita\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in e:\\sushmita\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in e:\\sushmita\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12acaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4dfaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a statement:\n",
      "\tcommonwealth bank cuts fixed home loan rates community urged to help homeless youth\n",
      "commonwealth bank cuts fixed home loan rates community urged to help homeless youth\n",
      "83\n",
      "\n",
      "Feature Names(TF-IDF):\n",
      " ['bank' 'commonwealth' 'community' 'cuts' 'fixed' 'help' 'home' 'homeless'\n",
      " 'loan' 'rates' 'to' 'urged' 'youth']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.2773501 0.2773501 0.2773501 0.2773501 0.2773501 0.2773501 0.2773501\n",
      "  0.2773501 0.2773501 0.2773501 0.2773501 0.2773501 0.2773501]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text=input('Enter a statement:\\n\\t')\n",
    "sent_token=sent_tokenize(text)\n",
    "#print('Sentences:\\n',sent_token)\n",
    "for sent in sent_token:\n",
    " print(sent)\n",
    " print(len(sent))\n",
    " \n",
    "print()\n",
    "tfidf=TfidfVectorizer()\n",
    "x_tfidf=tfidf.fit_transform(sent_token)\n",
    "features_names_tfidf=tfidf.get_feature_names_out()\n",
    "print('Feature Names(TF-IDF):\\n',features_names_tfidf)\n",
    "print()\n",
    "print('TF-IDF Matrix:\\n',x_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f964e",
   "metadata": {},
   "source": [
    "# N-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be077e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:big hopes for launceston cycling championship\n",
      "\n",
      "Generated 2-grams:\n",
      "('big', 'hopes')\n",
      "('hopes', 'for')\n",
      "('for', 'launceston')\n",
      "('launceston', 'cycling')\n",
      "('cycling', 'championship')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "text='big hopes for launceston cycling championship'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "#Generate bigrams\n",
    "n=2\n",
    "bigrams=list(ngrams(tokens,n))\n",
    "print(f\"Original text:{text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in bigrams:\n",
    " print(gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2374805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:aba decides against community broadcasting licence\n",
      "\n",
      "Generated 3-grams:\n",
      "('aba', 'decides', 'against')\n",
      "('decides', 'against', 'community')\n",
      "('against', 'community', 'broadcasting')\n",
      "('community', 'broadcasting', 'licence')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "text='aba decides against community broadcasting licence'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "#Generate n-grams\n",
    "n=3\n",
    "ngrams=list(ngrams(tokens,n))\n",
    "print(f\"Original text:{text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in ngrams:\n",
    " print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789f11b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:big hopes for launceston cycling championship\n",
      "\n",
      "Generated 4-grams:\n",
      "('big', 'hopes', 'for', 'launceston')\n",
      "('hopes', 'for', 'launceston', 'cycling')\n",
      "('for', 'launceston', 'cycling', 'championship')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "text='big hopes for launceston cycling championship'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "#Generate n-grams\n",
    "n=4\n",
    "ngrams=list(ngrams(tokens,n))\n",
    "print(f\"Original text:{text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in ngrams:\n",
    " print(gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095e0ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:big hopes for launceston cycling championship\n",
      "\n",
      "Generated 2-grams:\n",
      "['big', 'hopes']\n",
      "['hopes', 'for']\n",
      "['for', 'launceston']\n",
      "['launceston', 'cycling']\n",
      "['cycling', 'championship']\n"
     ]
    }
   ],
   "source": [
    "#generate n-grams with list comprehension\n",
    "n=2\n",
    "text='big hopes for launceston cycling championship'\n",
    "tokens=nltk.word_tokenize(text)\n",
    "ngrams=[tokens[i:i+n] for i in range(len(tokens)-n+1)]\n",
    "print(f\"Original text:{text}\")\n",
    "print()\n",
    "print(f\"Generated {n}-grams:\")\n",
    "for gram in ngrams:\n",
    " print(gram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789863d",
   "metadata": {},
   "source": [
    "# onehot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acd753f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.])]\n",
      "[array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "text=['aba decides against community broadcasting licence','act fire witnesses must be aware of defamation',\n",
    "      'ambitious olsson wins triple jump']\n",
    "#step 1- tokens\n",
    "tokens=[word for sent in text for word in sent.lower().split()]\n",
    "#step 2- vocabulary\n",
    "vocabulary=list(set(tokens)) # unique words in the text\n",
    "#initialize encoder\n",
    "encoder=OneHotEncoder(categories=[vocabulary],sparse=False)\n",
    "#Perform the one-hot encoding\n",
    "one_hot_encoded=[]\n",
    "for sent in text:\n",
    "    sent_encoded=[]\n",
    "    for word in sent.lower().split():\n",
    "        word_index=vocabulary.index(word)\n",
    "        word_vector=np.zeros(len(vocabulary))\n",
    "        word_vector[word_index]=1\n",
    "        sent_encoded.append(word_vector)\n",
    "        one_hot_encoded.append(sent_encoded)\n",
    " \n",
    "for sent in one_hot_encoded:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "985b89c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7709af97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e689eee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "995f34ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e577433",
   "metadata": {},
   "source": [
    "# Word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc691ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in e:\\sushmita\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in e:\\sushmita\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in e:\\sushmita\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in e:\\sushmita\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pandas in e:\\sushmita\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 0.0/67.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 67.1/67.1 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in e:\\sushmita\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\sushmita\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting simpful (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for simpful from https://files.pythonhosted.org/packages/8d/93/8448d3f1aa9d2911b8cba2602aaa1af85eb31a26d28b7b737f1fa5b40c02/simpful-2.11.1-py3-none-any.whl.metadata\n",
      "  Downloading simpful-2.11.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.5 in e:\\sushmita\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading simpful-2.11.1-py3-none-any.whl (32 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20448 sha256=cadf7ece034f9c39ac3fb4612612bc9e0f3bebfbd50eee03f9b263437626c480\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\69\\f5\\e5\\18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3522 sha256=9cc5ec6474d46658591a71d050512f08f6065f838a70ffeb83766eaa535ddc17\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\9d\\ff\\2f\\afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.2.25 simpful-2.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b62b6cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\sushmita\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in e:\\sushmita\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in e:\\sushmita\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\sushmita\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in e:\\sushmita\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in e:\\sushmita\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19c99de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'youth':[-0.04309844  0.01832869  0.02594942  0.02870969  0.03733459 -0.03083838\n",
      "  0.00552807  0.03023641 -0.01420025 -0.03086761 -0.00205112 -0.04184474\n",
      " -0.02800006  0.03552269  0.0167627   0.03612835  0.03400124  0.03765371\n",
      " -0.01894577 -0.00280903]\n",
      "====================================================================================================\n",
      "Vector for 'risk':[-0.00788826  0.00160686 -0.02070315 -0.03841344 -0.00754004  0.01234897\n",
      " -0.00444013  0.02766831 -0.01371489  0.01130033  0.02727897  0.04172977\n",
      " -0.0072687  -0.04604071  0.02185276  0.00285892  0.03720954 -0.00406641\n",
      " -0.01319207 -0.04376505]\n",
      "====================================================================================================\n",
      "Similarity b/w 'word' and 'successive':0.02916492521762848\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#sample text\n",
    "text=['freedom records net profit for third successive',\n",
    " 'funds allocated for domestic violence victims',\n",
    " 'funds allocated for youth at risk']\n",
    "tokenized_text=[word_tokenize(sentence.lower()) for sentence in text]\n",
    "#Train Word2vec model\n",
    "model=Word2Vec(sentences=tokenized_text,vector_size=20,window=5,min_count=1,workers=4)\n",
    "#Find word vectors\n",
    "vector_youth=model.wv['youth']\n",
    "vector_risk=model.wv['risk']\n",
    "#similarity b/w words\n",
    "similarity=model.wv.similarity('funds','successive')\n",
    "print(f\"Vector for 'youth':{vector_youth}\")\n",
    "print('='*100)\n",
    "print(f\"Vector for 'risk':{vector_risk}\")\n",
    "print('='*100)\n",
    "print(f\"Similarity b/w 'word' and 'successive':{similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee27f5",
   "metadata": {},
   "source": [
    "# Doc2Vec:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f0b909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['freedom', 'records', 'net', 'profit', 'for', 'third', 'successive'], tags=['0']), TaggedDocument(words=['funds', 'allocated', 'for', 'domestic', 'violence', 'victims'], tags=['1']), TaggedDocument(words=['funds', 'allocated', 'for', 'youth', 'at', 'risk'], tags=['2'])]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "#sample text\n",
    "documents=['freedom records net profit for third successive',\n",
    " 'funds allocated for domestic violence victims',\n",
    " 'funds allocated for youth at risk']\n",
    "#Tokenize & tag documents\n",
    "tagged_data=[TaggedDocument(words=word_tokenize(doc.lower()),\n",
    " tags=[str(i)]) for i,doc in enumerate(documents)]\n",
    "print(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc04c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Doc2vec model\n",
    "model=Doc2Vec(vector_size=100,window=2,min_count=1,workers=5,epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data,total_examples=model.corpus_count,epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e452d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_doc_1=model.infer_vector(word_tokenize(\"freedom records net profit for third successive\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6535ec28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.9496763e-03,  3.1143485e-03,  2.7254957e-03, -1.7791928e-03,\n",
       "        4.0919718e-04, -4.1887192e-03, -1.6309367e-03, -1.6367417e-03,\n",
       "        3.1562506e-03, -1.8598560e-03,  3.1855521e-03, -4.3522506e-03,\n",
       "       -1.1124995e-03, -1.0317671e-03,  5.0594512e-04, -2.1773055e-03,\n",
       "       -3.8218382e-03,  1.7934815e-03, -2.1344903e-03, -2.7294210e-03,\n",
       "       -5.0259515e-04,  3.1099967e-03, -2.4377510e-03, -2.2734059e-03,\n",
       "       -9.1357324e-05,  2.9990466e-03, -3.7868416e-03, -1.6672349e-03,\n",
       "        3.5287319e-03, -4.2792954e-03, -2.7764883e-04, -1.7303550e-03,\n",
       "        2.6221753e-03, -1.8597767e-03, -8.8109425e-04, -4.2428751e-03,\n",
       "        2.8178556e-04,  7.5170200e-04,  2.7657275e-03,  2.7228573e-03,\n",
       "       -4.6466233e-04, -2.6687635e-03, -4.0061614e-03,  7.3689659e-04,\n",
       "        2.5591089e-03,  4.0506478e-03, -2.2260456e-03,  3.3218581e-03,\n",
       "        3.7739768e-03,  2.1049415e-03, -1.9437169e-03,  2.3372011e-06,\n",
       "        9.5767906e-04, -1.5174360e-03, -7.9578266e-04, -2.5370012e-03,\n",
       "       -4.3664537e-03,  1.2438516e-03, -9.1554981e-04,  2.0929915e-03,\n",
       "        3.2901191e-03, -1.8868259e-03,  3.0873457e-03, -2.6674664e-03,\n",
       "        2.6243629e-03, -2.5254900e-03, -2.7414765e-03, -2.7392359e-04,\n",
       "        4.4852654e-03,  4.8904731e-03, -2.2637265e-04, -9.0867921e-04,\n",
       "       -3.4285798e-03,  2.7456109e-03,  1.7430078e-03, -2.2328475e-03,\n",
       "        1.3305682e-03, -5.0043864e-03,  5.4449931e-04,  3.0805483e-03,\n",
       "       -3.4847017e-03,  4.5609339e-03, -2.8119122e-03, -1.3674353e-03,\n",
       "        4.7952062e-03, -3.8478186e-03, -2.2229827e-03,  4.8042974e-03,\n",
       "        1.1792652e-03, -1.3012824e-03, -2.7911514e-03, -1.5453820e-03,\n",
       "       -3.1015244e-03, -2.5808460e-03, -1.3236640e-03, -2.9280558e-03,\n",
       "        3.4041468e-03, -5.8283418e-04,  1.0088671e-03,  1.9483126e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_doc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f4eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector for 'freedom records net profit for third successive':[-4.9496763e-03  3.1143485e-03  2.7254957e-03 -1.7791928e-03\n",
      "  4.0919718e-04 -4.1887192e-03 -1.6309367e-03 -1.6367417e-03\n",
      "  3.1562506e-03 -1.8598560e-03  3.1855521e-03 -4.3522506e-03\n",
      " -1.1124995e-03 -1.0317671e-03  5.0594512e-04 -2.1773055e-03\n",
      " -3.8218382e-03  1.7934815e-03 -2.1344903e-03 -2.7294210e-03\n",
      " -5.0259515e-04  3.1099967e-03 -2.4377510e-03 -2.2734059e-03\n",
      " -9.1357324e-05  2.9990466e-03 -3.7868416e-03 -1.6672349e-03\n",
      "  3.5287319e-03 -4.2792954e-03 -2.7764883e-04 -1.7303550e-03\n",
      "  2.6221753e-03 -1.8597767e-03 -8.8109425e-04 -4.2428751e-03\n",
      "  2.8178556e-04  7.5170200e-04  2.7657275e-03  2.7228573e-03\n",
      " -4.6466233e-04 -2.6687635e-03 -4.0061614e-03  7.3689659e-04\n",
      "  2.5591089e-03  4.0506478e-03 -2.2260456e-03  3.3218581e-03\n",
      "  3.7739768e-03  2.1049415e-03 -1.9437169e-03  2.3372011e-06\n",
      "  9.5767906e-04 -1.5174360e-03 -7.9578266e-04 -2.5370012e-03\n",
      " -4.3664537e-03  1.2438516e-03 -9.1554981e-04  2.0929915e-03\n",
      "  3.2901191e-03 -1.8868259e-03  3.0873457e-03 -2.6674664e-03\n",
      "  2.6243629e-03 -2.5254900e-03 -2.7414765e-03 -2.7392359e-04\n",
      "  4.4852654e-03  4.8904731e-03 -2.2637265e-04 -9.0867921e-04\n",
      " -3.4285798e-03  2.7456109e-03  1.7430078e-03 -2.2328475e-03\n",
      "  1.3305682e-03 -5.0043864e-03  5.4449931e-04  3.0805483e-03\n",
      " -3.4847017e-03  4.5609339e-03 -2.8119122e-03 -1.3674353e-03\n",
      "  4.7952062e-03 -3.8478186e-03 -2.2229827e-03  4.8042974e-03\n",
      "  1.1792652e-03 -1.3012824e-03 -2.7911514e-03 -1.5453820e-03\n",
      " -3.1015244e-03 -2.5808460e-03 -1.3236640e-03 -2.9280558e-03\n",
      "  3.4041468e-03 -5.8283418e-04  1.0088671e-03  1.9483126e-04]\n",
      "\n",
      "Most similar document:[('1', 0.10352430492639542), ('0', 0.09760893136262894), ('2', -0.08960097283124924)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15852\\418508219.py:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_doc=model.docvecs.most_similar(positive=[vector_doc_1])\n"
     ]
    }
   ],
   "source": [
    "#find the most similar document\n",
    "similar_doc=model.docvecs.most_similar(positive=[vector_doc_1])\n",
    "print(f\"vector for 'freedom records net profit for third successive':{vector_doc_1}\")\n",
    "print()\n",
    "print(f\"Most similar document:{similar_doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693cd066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
